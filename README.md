# AI Response Evaluation Framework

This project evaluates the quality of responses generated by different AI models, specifically GPT and Gemini, for a set of user questions. The evaluation is based on a predefined prompt that scores the responses on a scale of 1 to 4, considering the relevance, completeness, and helpfulness of the answers.

## Setup

1. **Install Dependencies**:
    ```sh
    %pip install -r requirements.txt
    ```

2. **Load Environment Variables**:
    Ensure that the `.env` file contains the necessary API keys for OpenAI and Google Gemini:
    ```
    OPENAI_API_KEY=your_openai_api_key
    GOOGLE_API_KEY=your_google_api_key
    ```

## Process

1. **Load Data**:
    ```python
    df = pd.read_csv('QuestionSet/Q4.csv')
    df['Category'] = df['Category'].str.lower()
    ```

2. **Clean Data**:
    Drop rows with missing questions.
    ```python
    df = df.dropna(subset=['Question'])
    ```

3. **Evaluate Responses**:
    Evaluate the responses using the predefined prompt.
    ```python
    def evaluate_response(idx, question, answer, prompt_template):
        prompt = prompt_template.format(question=question, answer=answer)
        response = model(text=prompt)
        return idx, response['message']
    ```

4. **Review and Rate**:
    Generate reviews and ratings for both GPT and Gemini responses.
    ```python
    gpt_review = []
    gemini_review = []
    # Loop through the dataframe to evaluate
    for i in range(len(df)):
        question = df.iloc[i]['Question']
        answer = df.iloc[i]['GPT_Answer']
        idx, review = evaluate_response(i, question, answer, IMPROVED_JUDGE_PROMPT)
        gpt_review.append(review)
    df['GPT_Review'] = gpt_review
    
    for i in range(len(df)):
        question = df.iloc[i]['Question']
        answer = df.iloc[i]['Gemini_Answer']
        idx, review = evaluate_response(i, question, answer, IMPROVED_JUDGE_PROMPT)
        gemini_review.append(review)
    df['Gemini_Review'] = gemini_review
    ```

5. **Calculate Ratings**:
    Extract ratings from the reviews.
    ```python
    df['GPT_Rating'] = df['GPT_Review'].apply(lambda x: int(x.split('Total rating: ')[1][0]))
    df['Gemini_Rating'] = df['Gemini_Review'].apply(lambda x: int(x.split('Total rating: ')[1][0]))
    ```

6. **Save Evaluated Data**:
    Save the evaluated data to a CSV file.
    ```python
    df.to_csv('QuestionSet/Q4_evaluated.csv', index=False)
    ```

7. **Visualize Results**:
    Plot the average ratings by category.
    ```python
    import matplotlib.pyplot as plt
    average_ratings1 = df.groupby(['Category'])['GPT_Rating'].mean()
    average_ratings2 = df.groupby(['Category'])['Gemini_Rating'].mean()

    plt.figure(figsize=(10, 6))
    average_ratings1.plot(kind='bar', alpha=0.75, label='GPT')
    average_ratings2.plot(kind='bar', alpha=0.75, color='orange', label='Gemini')

    plt.ylabel('Average Rating')
    plt.title('Average Rating by Category')
    plt.legend()
    plt.show()
    ```

8. **Analyze Low Ratings**:
    Identify and save questions with low ratings.
    ```python
    low_rating_threshold = 1
    low_rating_questions = df[(abs(df['GPT_Rating'] - df['Gemini_Rating']) > low_rating_threshold) | 
                              (df['GPT_Rating'] <= low_rating_threshold) | 
                              (df['Gemini_Rating'] <= low_rating_threshold)]
    low_rating_questions = pd.concat([low_rating_questions, df.sample(5)])
    low_rating_questions.to_csv('QuestionSet/Q4_low_rating_questions.csv', index=False)
    ```

## Conclusion

This project provides a comprehensive evaluation of AI-generated responses, comparing different models and visualizing their performance across various categories. The methodology ensures an objective assessment of response quality, aiding in the refinement and improvement of AI systems.

For more detailed insights, please refer to the accompanying [report](https://docs.google.com/document/d/1TqBCF8SqOHfkeu5KBqpHZs7vN7lm76rzQo1ss2FC1uw).
